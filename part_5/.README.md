## Mixture-of-Experts (MoE):
Overview

This module implements the Mixture-of-Experts (MoE) feed-forward layer, a core component in modern sparse transformers.

Important: This part focuses only on the feed-forward component — self-attention is not implemented here.

In a standard transformer block, the order is:

[LayerNorm] → [Self-Attention] → [Residual Add]
           → [LayerNorm] → [Feed-Forward Block (Dense or MoE)] → [Residual Add]


Our MoE layer acts as a drop-in replacement for the dense feed-forward block. It is typically called after attention outputs, just before the second residual connection.

1. Theory in 60 Seconds

Experts

Multiple parallel MLPs.

Each token activates only a small subset (top-k) → sparse computation.

Gate/Router

Scores each token across experts.

Picks the top-k experts and assigns weights via a softmax.

Dispatch / Combine

Sends tokens to chosen experts.

Runs the expert MLPs.

Combines results using the gate weights.

Load Balancing

Encourages uniform usage of experts.

Example auxiliary loss (Switch Transformer):

𝐿
aux
=
𝐸
⋅
∑
(
importance
⋅
load
)
L
aux
	​

=E⋅∑(importance⋅load)

where:

importance = avg gate probability per expert

load = fraction of tokens routed as primary to that expert

2. Implementation Notes
2.1 Distributed / Single-GPU Friendly

Real MoE implementations often distribute experts across GPUs (expert parallelism).

This module keeps everything on one device for simplicity.

Dispatching is simulated via indexing/masking.

In production, dispatch/combination usually requires all-to-all communication across devices.

2.2 Hybrid Architectures

MoE does not need to replace every feed-forward network (FFN).

Can be used in alternating layers or blended with dense FFNs:

𝑦
=
𝛼
⋅
Dense
(
𝑥
)
+
(
1
−
𝛼
)
⋅
MoE
(
𝑥
)
y=α⋅Dense(x)+(1−α)⋅MoE(x)
3. Milestone

Integrate this MoE layer in place of a dense feed-forward in a transformer block.

Compare efficiency and accuracy trade-offs.

Can be tested with a toy attention block if needed.

4. Usage Example
import torch
from moe_module import MoELayer

# Example: input tensor [batch_size, seq_len, hidden_dim]
x = torch.randn(32, 128, 512)

# Create MoE layer with 4 experts and top-2 routing
moe = MoELayer(hidden_dim=512, num_experts=4, top_k=2)

# Forward pass
output, aux_loss = moe(x)

5. References

Shazeer et al., “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer”, 2017.

Lepikhin et al., “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding”, 2020.

Fedus et al., “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity”, 2021.
